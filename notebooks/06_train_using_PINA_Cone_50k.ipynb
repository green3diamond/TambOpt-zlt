{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c137a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_with_cone = pd.read_parquet('../ml/processed_events_50k/raw_cone_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from ml.generate_cone_data import normalize_cone_features\n",
    "train_data_with_cone = normalize_cone_features(train_data_with_cone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from src.plotting import plot_2d_scatter, plot_histogram, plot_records_per_plane, plot_cone_with_scatter, plot_y_values_per_plane, generate_3d_cone_animation\n",
    "from src.data_exploration import calculate_max_records_events, select_event_data, print_y_values_per_plane\n",
    "from src.featurization import calcualte_cone_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e5a94",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31053012",
   "metadata": {},
   "source": [
    "Select input and output columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5cdfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_columns = [\n",
    "            \"primary_kinetic_energy\",\n",
    "            \"sin_azimuth\", \"cos_azimuth\", \"sin_zenith\", \"cos_zenith\"\n",
    "        ]\n",
    "output_columns = [\n",
    "    'X_mean_min', 'Y_mean_min', 'Z_mean_min',\n",
    "    'X_mean_max', 'Y_mean_max', 'Z_mean_max',\n",
    "    'radius'\n",
    "]\n",
    "batch_identifier = \"event_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a67f7",
   "metadata": {},
   "source": [
    "Calculate meand and std of secondary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef385e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_cone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbec3f4",
   "metadata": {},
   "source": [
    "Impute nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate na values before imputation\n",
    "print(f\"NA values before imputation:\\n{train_data_with_cone.isnull().sum()}\")\n",
    "\n",
    "# impute na with 0\n",
    "train_data_with_cone = train_data_with_cone.dropna()\n",
    "\n",
    "# validate na values after imputation\n",
    "print(f\"NA values after imputation:\\n{train_data_with_cone.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e81206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Event stats columns: {train_data_with_cone.columns}\")\n",
    "print(f\"Input columns: {input_columns}\")\n",
    "print(f\"Output columns: {output_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fab884",
   "metadata": {},
   "source": [
    "# Define NN using PINA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f8d2f",
   "metadata": {},
   "source": [
    "Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split inputs and outputs \n",
    "X = train_data_with_cone[input_columns]\n",
    "y = train_data_with_cone[output_columns]\n",
    "\n",
    "\n",
    "# y_normalized = y - y.mean()\n",
    "# y_max = y_normalized.max().max()\n",
    "# y_normalized = y_normalized / y_max\n",
    "\n",
    "# create scaler for each output column\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# fit and transform each column separately\n",
    "y_normalized = y_scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f553334",
   "metadata": {},
   "source": [
    "PINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# temp_input = pd.concat([y_normalized, X])\n",
    "# # columns to exclude from the pairplot\n",
    "# exclude_cols = ['event_id', 'primary_kinetic_energy', 'min_plane', 'max_plane']\n",
    "\n",
    "# # build dataframe for plotting (only numeric columns)\n",
    "# plot_df = temp_input.drop(columns=[c for c in exclude_cols if c in temp_input.columns])\n",
    "# plot_df = plot_df.select_dtypes(include='number')\n",
    "\n",
    "# sns.pairplot(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pina import Trainer, Condition, LabelTensor\n",
    "from pina.problem import AbstractProblem\n",
    "from src.nn import Model\n",
    "from pina.optim import TorchOptimizer, TorchScheduler\n",
    "from pina.solver import DeepEnsembleSupervisedSolver\n",
    "from pina.callback import MetricTracker\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pina = LabelTensor(X.values, input_columns)\n",
    "y_pina = LabelTensor(y_normalized, output_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianProblem(AbstractProblem):\n",
    "\n",
    "    output_variables = output_columns\n",
    "    input_variables = input_columns\n",
    "    conditions = {\"data\": Condition(input=x_pina, target=y_pina)}\n",
    "\n",
    "\n",
    "problem = BayesianProblem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b55389",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 2\n",
    "\n",
    "# define problem & data (step 1)\n",
    "\n",
    "\n",
    "models = [\n",
    "  Model(\n",
    "      input_dimensions=len(problem.input_variables),\n",
    "      output_dimensions=len(problem.output_variables),\n",
    "      layers=[100, 100, 100],\n",
    "      func=torch.nn.Tanh\n",
    "  )\n",
    "  for _ in range(n_models)\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "     TorchOptimizer(torch.optim.RAdam, lr=0.005)\n",
    "     for _ in range(n_models)\n",
    "]\n",
    "\n",
    "schedulers = [\n",
    "    TorchScheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=[50, 100, 300, 500, 800], gamma=0.5)\n",
    "    for _ in range(n_models)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fa4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DeepEnsembleSupervisedSolver(\n",
    "    problem,\n",
    "    models,\n",
    "    optimizers=optimizers,\n",
    "    schedulers=schedulers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1073b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer\n",
    "trainer = Trainer(\n",
    "    solver=solver,  # The ensemble solver\n",
    "    max_epochs=1500,  # Maximum number of training epochs\n",
    "    logger=True,  # Enables logging (default logger is CSVLogger)\n",
    "    callbacks=[MetricTracker()],  # Tracks training metrics using MetricTracker\n",
    "    accelerator=\"cpu\",  # Use CPU for training, alternative is \"gpu\" for GPU training\n",
    "    train_size=0.7,  # Fraction of the dataset used for training (70%)\n",
    "    test_size=0.2,  # Fraction of the dataset used for testing (20%)\n",
    "    val_size=0.1,  # Fraction of the dataset used for validation (10%)\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# train\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22cf3c",
   "metadata": {},
   "source": [
    "Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting final loss\n",
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c71c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "trainer_metrics = trainer.callbacks[0].metrics\n",
    "loss = trainer_metrics[\"train_loss\"]\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss.cpu())\n",
    "# plotting\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd0ed0",
   "metadata": {},
   "source": [
    "# Test NN: Plot errors in output/target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = None\n",
    "all_targets = None\n",
    "\n",
    "trainer.data_module.setup(\"test\")\n",
    "with torch.no_grad():\n",
    "    for data in trainer.data_module.test_dataloader():\n",
    "    # for data in trainer.data_module.train_dataloader():\n",
    "        inputs, target = data[0][1][\"input\"], data[0][1][\"target\"]\n",
    "        outputs = solver(inputs)\n",
    "        \n",
    "        if all_outputs is None:\n",
    "            all_outputs = LabelTensor(outputs, labels=output_columns)\n",
    "            all_targets = target\n",
    "        else:\n",
    "            all_outputs.append(LabelTensor(outputs, labels=output_columns))\n",
    "            all_targets.append(target)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot targets vs predictions for validation set\n",
    "y_mean, y_std = all_outputs.mean(0).detach(), all_outputs.std(0).detach()\n",
    "true_output = all_targets.detach()\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "# use 3 columns per row\n",
    "for i, col in enumerate(output_columns):\n",
    "    plt.subplot(len(output_columns)//4+1, 4, i+1)\n",
    "    plt.scatter(true_output[:, i], y_mean[:, i], alpha=0.5)#, s=20*y_std[:, i]/y_std.max())\n",
    "    plt.plot([true_output[:, i].min(), true_output[:, i].max()],\n",
    "             [true_output[:, i].min(), true_output[:, i].max()], 'r--')\n",
    "    \n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(col)\n",
    "plt.tight_layout(pad=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
