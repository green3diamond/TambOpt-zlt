{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ebe0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T04:09:06.885628Z",
     "start_time": "2026-01-13T04:09:06.802623Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2026-02-03T13:59:11.981142",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897e81f-487a-489c-ad2f-3b131df37c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T04:09:07.807278Z",
     "start_time": "2026-01-13T04:09:06.887281Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.colors import Normalize\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6870de-be4e-4af6-af2b-ca6a35a70ff2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98c61d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Ready Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0f45a-17ca-48d7-9e7a-a07bf4ee9c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T04:09:07.889654Z",
     "start_time": "2026-01-13T04:09:07.843798Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function evaluates the specific angular coefficients at given energy value\n",
    "\n",
    "def Y(energy, f_mode):\n",
    "    \"\"\"Evaluate angular coefficient vector Y for given energy and mode.\n",
    "\n",
    "    Parameters:\n",
    "        energy (torch.Tensor): input energy (expected in PeV-range; scalar or 1-element tensor).\n",
    "        f_mode (str): mode string selecting parameterization (e.g. 'mg0','mg2','eg0',...).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (4, 1) tensor with evaluated coefficients used by interpolation routines.\n",
    "    \"\"\"\n",
    "    Y = torch.zeros((4, 1))\n",
    "\n",
    "    #Convert Energy to PeV\n",
    "    xe = .5 + 20 * (torch.log(energy) - log_01) / (log_10 - log_01)\n",
    "    xe2 = xe * xe\n",
    "\n",
    "    #Evaluate Y for given energy and mode\n",
    "    for i in range(4):\n",
    "        if f_mode == \"mg0\":\n",
    "            Y[i] = torch.exp(PXmg_p[i, 0, 0]) + torch.exp(PXmg_p[i ,0, 1] * torch.pow(xe, PXmg_p[i, 0, 2]))\n",
    "\n",
    "        elif f_mode == \"mg2\":\n",
    "            Y[i] = PXmg_p[i, 2, 0] + PXmg_p[i, 2, 1] * xe + PXmg_p[i, 2, 2] * xe2\n",
    "\n",
    "        elif f_mode == \"eg0\":\n",
    "            Y[i] = PXeg_p[i, 0, 0] * torch.exp(PXeg_p[i, 0, 1] * torch.pow(xe, PXeg_p[i, 0, 2]))\n",
    "\n",
    "        elif f_mode == \"eg1\":\n",
    "            Y[i] = PXeg_p[i, 1, 0] + PXeg_p[i, 1, 1] * xe + PXeg_p[i, 1, 2] * xe2\n",
    "\n",
    "        elif f_mode == \"eg2\":\n",
    "            Y[i] = PXeg_p[i, 2, 0] + PXeg_p[i, 2, 1] * xe + PXeg_p[i, 2, 2] * xe2\n",
    "\n",
    "        elif f_mode == \"mp0\":\n",
    "            Y[i] = torch.exp(PXmp_p[i, 0, 0]) + torch.exp(PXmp_p[i, 0, 1] * torch.pow(xe, PXmp_p[i, 0, 2]))\n",
    "\n",
    "        elif f_mode == \"mp2\":\n",
    "            Y[i] = PXmp_p[i, 2, 0] + PXmp_p[i, 2, 1] * xe + PXmp_p[i, 2, 2] * xe2\n",
    "\n",
    "        elif f_mode == \"ep0\":\n",
    "            Y[i] = torch.exp(PXep_p[i, 0, 0]) + torch.exp(PXep_p[i, 0, 1] * torch.pow(xe, PXep_p[i, 0, 2]))\n",
    "\n",
    "        elif f_mode == \"ep1\":\n",
    "            Y[i] = PXep_p[i, 1, 0] + PXep_p[i, 1, 1] * xe + PXep_p[i, 1, 2] * xe2\n",
    "\n",
    "        elif f_mode == \"ep2\":\n",
    "            Y[i] = PXep_p[i, 2, 0] + PXep_p[i, 2, 1] * xe + PXep_p[i, 2, 2] * xe2\n",
    "\n",
    "        else:\n",
    "            warnings.warn(\"The Mode is not defined\")\n",
    "            return\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d117d38-bb9b-4aca-9ce8-7c7a929bab51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T04:09:07.908528Z",
     "start_time": "2026-01-13T04:09:07.892333Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Interpolated parameter computation\n",
    "#thisp0 = normalization constant\n",
    "#thisp1 = slope parameter\n",
    "#thisp2 = shape parameter\n",
    "\n",
    "def thisp(energy, theta, f_mode):\n",
    "    \"\"\"Compute interpolated parameter value for a given energy, incident angle and parameter mode.\n",
    "\n",
    "    The function evaluates the angular coefficients via `Y`, solves a small linear system to get\n",
    "    polynomial coefficients `B`, then evaluates a polynomial in `x(theta)` to return a single parameter.\n",
    "\n",
    "    Parameters:\n",
    "        energy (torch.Tensor): shower energy.\n",
    "        theta (torch.Tensor): shower incident angle.\n",
    "        f_mode (str): mode string (e.g. 'mg0', 'eg1', 'mp2').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: interpolated scalar parameter value used in the flux parametrization.\n",
    "    \"\"\"\n",
    "    #Find Y values\n",
    "    Y_val = Y(energy, f_mode)\n",
    "\n",
    "    #Solve for B\n",
    "    B = torch.linalg.solve(A, Y_val)\n",
    "\n",
    "    #Define x from theta\n",
    "    x = .5 + 4 * theta / theta_max\n",
    "\n",
    "    res = 0\n",
    "\n",
    "    for i in range(4):\n",
    "        res += B[i] * x ** i\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312dd3c8-23ee-4869-848a-faa53a0df6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T04:09:07.931113Z",
     "start_time": "2026-01-13T04:09:07.909354Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ReadShowers(path_g, path_p):\n",
    "    \"\"\"Read fitted parameter blocks for gamma and proton showers from text files.\n",
    "\n",
    "    Expects both files to contain 4 parameter blocks for electrons and muons (each block of 3 rows\n",
    "    and 3 columns). Performs a simple validation (warns and returns None if a block contains zeros)\n",
    "    and returns the parameter tensors as torch.Tensor objects.\n",
    "\n",
    "    Parameters:\n",
    "        path_g (str): path to gamma-fit text file.\n",
    "        path_p (str): path to proton-fit text file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (PXmg_p, PXeg_p, PXmp_p, PXep_p) each a torch.Tensor of shape (4,3,3), or None on error.\n",
    "    \"\"\"\n",
    "    #GAMMA SHOWERS\n",
    "    #Reading the Electron Parameters in the Showers\n",
    "    PXeg1_p = np.loadtxt(path_g, max_rows = 3)\n",
    "\n",
    "    for i in range(3):\n",
    "        if PXeg1_p[i, 0]*PXeg1_p[i, 1]*PXeg1_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXeg2_p = np.loadtxt(path_g, skiprows = 3, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXeg2_p[i, 0]*PXeg2_p[i, 1]*PXeg2_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXeg3_p = np.loadtxt(path_g, skiprows = 6, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXeg3_p[i, 0]*PXeg3_p[i, 1]*PXeg3_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXeg4_p = np.loadtxt(path_g, skiprows = 9, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXeg4_p[i, 0]*PXeg4_p[i, 1]*PXeg4_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "            \n",
    "    #Reading the Muon Parameters in the Showers\n",
    "    PXmg1_p = np.loadtxt(path_g, skiprows = 12, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmg1_p[i, 0]*PXmg1_p[i, 1]*PXmg1_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "            \n",
    "    PXmg2_p = np.loadtxt(path_g, skiprows = 15, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmg2_p[i, 0]*PXmg2_p[i, 1]*PXmg2_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXmg3_p = np.loadtxt(path_g, skiprows = 18, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmg3_p[i, 0]*PXmg3_p[i, 1]*PXmg3_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXmg4_p = np.loadtxt(path_g, skiprows = 21, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmg4_p[i, 0]*PXmg4_p[i, 1]*PXmg4_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    #PROTON SHOWERS\n",
    "    #Reading the Electron Parameters in the Showers\n",
    "    PXep1_p = np.loadtxt(path_p, max_rows = 3)\n",
    "\n",
    "    for i in range(3):\n",
    "        if PXep1_p[i, 0]*PXep1_p[i, 1]*PXep1_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXep2_p = np.loadtxt(path_p, skiprows = 3, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXep2_p[i, 0]*PXep2_p[i, 1]*PXep2_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXep3_p = np.loadtxt(path_p, skiprows = 6, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXep3_p[i, 0]*PXep3_p[i, 1]*PXep3_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXep4_p = np.loadtxt(path_p, skiprows = 9, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXep4_p[i, 0]*PXep4_p[i, 1]*PXep4_p[i, 2] == 0:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "            \n",
    "    #Reading the Muon Parameters in the Showers\n",
    "    PXmp1_p = np.loadtxt(path_p, skiprows = 12, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmp1_p[i, 0]*PXmp1_p[i, 1]*PXmp1_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXmp2_p = np.loadtxt(path_p, skiprows = 15, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmp2_p[i, 0]*PXmp2_p[i, 1]*PXmp2_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXmp3_p = np.loadtxt(path_p, skiprows = 18, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmp3_p[i, 0]*PXmp3_p[i, 1]*PXmp3_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXmp4_p = np.loadtxt(path_p, skiprows = 21, max_rows = 3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if PXmp4_p[i, 0]*PXmp4_p[i, 1]*PXmp4_p[i, 2] == 0 and i != 1:\n",
    "            warnings.warn(\"Encountered 0\")\n",
    "            return\n",
    "\n",
    "    PXmg_p = torch.tensor([PXmg1_p, PXmg2_p, PXmg3_p, PXmg4_p])\n",
    "    PXeg_p = torch.tensor([PXeg1_p, PXeg2_p, PXeg3_p, PXeg4_p])\n",
    "    PXmp_p = torch.tensor([PXmp1_p, PXmp2_p, PXmp3_p, PXmp4_p])\n",
    "    PXep_p = torch.tensor([PXep1_p, PXep2_p, PXep3_p, PXep4_p])\n",
    "\n",
    "    return PXmg_p, PXeg_p, PXmp_p, PXep_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42c5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T04:09:07.947810Z",
     "start_time": "2026-01-13T04:09:07.931799Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function defines the initial layouts of the detectors, N_det = 90\n",
    "#At this point of the project, we just define a square grid\n",
    "\n",
    "def Layouts(n_detectors=100, n_rings=6):\n",
    "    \"\"\"Create a detector layout with detectors distributed across concentric rings.\n",
    "\n",
    "    Parameters:\n",
    "        n_detectors (int): total number of detectors.\n",
    "        n_rings (int): number of concentric rings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x, y) numpy arrays of detector positions in meters.\n",
    "    \"\"\"\n",
    "    # Distribute detectors across rings proportional to circumference (i.e. radius)\n",
    "    # R = [50, 200, 350, 550, 750, 1000]\n",
    "    # R = [0.5, 2.0, 3.5, 5.5, 7.5, 10]\n",
    "    # R = [5, 20, 35, 55, 75, 100]\n",
    "    R = np.linspace(5, 100, n_rings)\n",
    "    \n",
    "    weights = R / R.sum()\n",
    "    N = np.round(weights * n_detectors).astype(int)\n",
    "\n",
    "    # Fix rounding so total matches exactly\n",
    "    diff = n_detectors - N.sum()\n",
    "    # Add/remove from the outermost ring\n",
    "    N[-1] += diff\n",
    "\n",
    "    radii = np.repeat(R, N)\n",
    "    angles = np.concatenate([np.linspace(0, 2 * np.pi, n, endpoint=False) for n in N])\n",
    "\n",
    "    return radii * np.cos(angles), radii * np.sin(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f402091",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Functions to redo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305399a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Instantiate generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a2d07",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9aa50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusion_model.tambo_3D_diffusion_generator import PlaneDiffusionEvaluator\n",
    "\n",
    "# Initialize with custom parameters\n",
    "generator = PlaneDiffusionEvaluator(\n",
    "    data_dir =  \"/n/netscratch/arguelles_delgado_lab/Everyone/hhanif/tambo_simulation_nov_25/pre_processed_3rd_step/\",\n",
    "    checkpoint_path =  \"/n/netscratch/arguelles_delgado_lab/Everyone/hhanif/tambo_simulation_nov_25/checkpoints/tam_unet/epoch_epoch=1229-val_loss_val_loss=0.0333.ckpt\",\n",
    "    ddim_steps =  10,\n",
    "    eta =  0.0,\n",
    "    guidance_w =  1.8,\n",
    "    imports_path = \"/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/hhanif/tambo_optimization/unet\"\n",
    "    )\n",
    "\n",
    "generator.load_model()\n",
    "\n",
    "# add scaling\n",
    "from diffusion_model.tambo_3D_fnn_scaler import PlaneFNNGenerator\n",
    "import torch\n",
    "\n",
    "# Initialize\n",
    "scaler = PlaneFNNGenerator(\n",
    "    data_dir=\"/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/zdimitrov/tambo_simulations/pre_processed_3rd_step_min_50/\",\n",
    "    checkpoint_path=\"/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/zdimitrov/tambo_simulations/checkpoints/tam_fnn/last.ckpt\",\n",
    "    output_dir=\"fnn_outputs\",\n",
    "    imports_path=\"/n/home05/zdimitrov/tambo/TambOpt/ml/scaling_NN/FNN/\"\n",
    ")\n",
    "scaler.load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf4b4a-b495-41cc-8a26-c704e57aafc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GenerateShowers(x, y, log=False):\n",
    "    \"\"\"Randomly generate the showers, getting the energy, angle, and position of the shower core\n",
    "\n",
    "    Parameters:\n",
    "        x (torch.Tensor): detector x positions.\n",
    "        y (torch.Tensor): detector y positions.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a)\n",
    "            N (torch.Tensor): per-detector counts (Ne + Nm).\n",
    "            T (torch.Tensor): per-detector time-of-arrival.\n",
    "            X0, Y0 (torch.Tensor): shower core position.\n",
    "            energy, sin_z, cos_z, sin_a, cos_a (torch.Tensor): shower truth parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    #Define the energy and angles\n",
    "    p_energy = torch.tensor([np.random.uniform(0, 1)], dtype = torch.float32)\n",
    "    sin_zenith = torch.tensor([np.random.uniform(-1, 1)], dtype = torch.float32)\n",
    "    sin_azimuth = torch.tensor([np.random.uniform(-1, 1)], dtype = torch.float32)\n",
    "    # split theta and phi in cos and sin components if needed\n",
    "    sin_z = sin_zenith\n",
    "    cos_z = torch.sqrt(1 - sin_zenith**2)\n",
    "    sin_a = sin_azimuth\n",
    "    cos_a = torch.sqrt(1 - sin_azimuth**2)\n",
    "    \n",
    "    # TODO hardcoded for now\n",
    "    class_id = torch.tensor([1.0], dtype = torch.float32) # for gamma showers\n",
    "    \n",
    "    # input parameters \n",
    "    generator.test_conditions = torch.tensor([[p_energy, class_id,  sin_z, cos_z, sin_a, cos_a]])\n",
    "    scaler.test_conditions = torch.tensor([[p_energy, class_id, sin_z, cos_z, sin_a, cos_a]])\n",
    "\n",
    "    # generate shower\n",
    "    outputs_arr = generator.generate_samples(num_samples=1, num_conditions=1, chunk_size=1)\n",
    "    output_images = outputs_arr[0]['images'].squeeze()\n",
    "    shower_rgb = output_images[20] # use plane 20 for now\n",
    "    shower_rgb = shower_rgb.permute(1,2,0)\n",
    "    \n",
    "    # generate bboxes\n",
    "    outputs_arr_bboxes = scaler.generate_samples(num_samples=1, num_conditions=1)\n",
    "    bboxes = outputs_arr_bboxes[0]['bboxes'][:,20,:]  # use plane 20 for now\n",
    "    \n",
    "    \n",
    "    if log:\n",
    "        plt.imshow(shower_rgb)\n",
    "        print('BBox:', bboxes )\n",
    "    \n",
    "    # compute x0, y0 after generating the shower from the mean of the distribution over x, y\n",
    "    location_means = torch.prod(shower_rgb[:,:,:2], dim=2)\n",
    "    # Create position grids\n",
    "    i_indices = torch.arange(32, dtype=torch.float32)\n",
    "    j_indices = torch.arange(32, dtype=torch.float32)\n",
    "    i_grid, j_grid = torch.meshgrid(i_indices, j_indices, indexing='ij')\n",
    "\n",
    "    # Calculate position-weighted mean coefficients\n",
    "    X0 = torch.sum(i_grid * location_means) / torch.sum(location_means)\n",
    "    X0 = X0 * (bboxes[0,1] - bboxes[0,0]) / 32 + bboxes[0,0]\n",
    "    Y0 = torch.sum(j_grid * location_means) / torch.sum(location_means)\n",
    "    Y0 = Y0 * (bboxes[0,3] - bboxes[0,2]) / 32 + bboxes[0,2]\n",
    "    \n",
    "    #Evalute the counts in the tanks\n",
    "    N, T = GetCounts(shower_rgb, x, y, bboxes)\n",
    "\n",
    "    return N, T, X0, Y0, p_energy, sin_z, cos_z, sin_a, cos_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f618617-32fe-4df1-be7f-b414ae70d467",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GetCounts(shower_rgb, x, y, bboxes):\n",
    "    \"\"\"Compute per-detector particle counts and times from a shower RGB map.\n",
    "    \n",
    "    Parameters:\n",
    "        shower_rgb: (32, 32, 3) tensor - spatial shower distribution on the ground.\n",
    "        x, y: detector positions (tensor or iterable).\n",
    "        bboxes: bounding box [x_min, y_min, x_max, y_max] in meters.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Ne + Nm, Te) - total counts per detector and arrival times.\n",
    "    \"\"\"\n",
    "    electron_scale_factor = 10.0  # scale factor to convert intensity to electron counts\n",
    "    c_light = 0.3  # speed of light in m/ns\n",
    "    \n",
    "    # Extract bbox bounds (assuming format: [x_min, y_min, x_max, y_max])\n",
    "    bbox = bboxes.squeeze()\n",
    "    x_min, x_max, y_min, y_max = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    bbox_width = x_max - x_min\n",
    "    bbox_height = y_max - y_min\n",
    "    # print(f\"BBox: x_min={x_min}, x_max={x_max}, y_min={y_min}, y_max={y_max}\")\n",
    "    \n",
    "    # Extract particle density/intensity from the third channel or overall intensity\n",
    "    # particle_density = shower_rgb.mean(dim=2)\n",
    "    particle_density_energy = torch.prod(shower_rgb[:,:,:2], dim=2)\n",
    "    # Extract arrival time from third channel\n",
    "    arrival_time_map = shower_rgb[:,:,2]\n",
    "    \n",
    "    Ne_list, Te_list = [], []\n",
    "\n",
    "    for idx in range(len(x)):\n",
    "        # Scale detector coordinates using bbox to find nearest pixel\n",
    "        x_idx = int(torch.clamp((x[idx] - x_min) / bbox_width * 32, 0, 31))\n",
    "        y_idx = int(torch.clamp((y[idx] - y_min) / bbox_height * 32, 0, 31))\n",
    "        if x_idx == 0 or x_idx == 31 or y_idx == 0 or y_idx == 31:\n",
    "            Ne_i = torch.tensor(0.0).unsqueeze(0)\n",
    "            \n",
    "        else:        \n",
    "        # Sample particle count from the RGB map at detector location\n",
    "            local_intensity = particle_density_energy[y_idx, x_idx]\n",
    "        \n",
    "        # Convert intensity to particle counts\n",
    "        e0 = local_intensity * electron_scale_factor\n",
    "\n",
    "        # Smear and add background\n",
    "        nes = SmearN(e0)\n",
    "        neb = SmearN(fluxB_e)\n",
    "\n",
    "        Ne_i = nes + neb\n",
    "        Ne_list.append(Ne_i)\n",
    "\n",
    "        # Use arrival time from the third channel of shower_rgb\n",
    "        et = arrival_time_map[y_idx, x_idx]\n",
    "\n",
    "        if Ne_i > 0:\n",
    "            TAe_m, TAe_s = TimeAverage(et, neb, nes)\n",
    "            Te_val = TAe_m + torch.randn_like(TAe_m) * TAe_s\n",
    "            Te_list.append(Te_val.reshape(()))\n",
    "        else:\n",
    "            Te_list.append(torch.tensor(0.0))\n",
    "\n",
    "    Ne = torch.stack(Ne_list)\n",
    "    Te = torch.stack(Te_list)\n",
    "\n",
    "    return Ne, Te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac5b33",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Rest of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49b929-46a3-4954-9a3f-cca738315e5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function evaluates the distance of the given unit from the shower core\n",
    "def EffectiveDistance(xd, yd, x0, y0, th, ph):\n",
    "    \"\"\"Compute effective lateral distance from shower axis to detector.\n",
    "\n",
    "    Uses geometry to project the detector position onto plane perpendicular to shower direction.\n",
    "\n",
    "    Parameters:\n",
    "        xd, yd: detector coordinates.\n",
    "        x0, y0: shower core coordinates.\n",
    "        th, ph: shower arrival angles.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: effective distance (clamped to R_min).\n",
    "    \"\"\"\n",
    "    dx = xd - x0\n",
    "    dy = yd - y0\n",
    "    t = torch.sin(th) * torch.cos(ph) * dx + torch.sin(th) * torch.sin(ph) * dy\n",
    "    r = dx ** 2 + dy ** 2 - t ** 2\n",
    "\n",
    "    r = torch.where(r > 0, torch.sqrt(r), torch.zeros_like(r))  # or any fallback like r=r\n",
    "    r = torch.clamp(r, min = R_min)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c118a-f7c3-4e54-85f5-bd828472e40f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function evaluates the time of arrival of the particles to the ground\n",
    "#t = 0 for first arrivals while t > 0 for late arrival particles\n",
    "\n",
    "def EffectiveTime(xd, yd, x0, y0, th, ph):\n",
    "    \"\"\"Compute expected time-of-arrival at a detector relative to a reference, based on shower geometry.\n",
    "\n",
    "    Parameters:\n",
    "        xd, yd: detector coordinates.\n",
    "        x0, y0: shower core coordinates.\n",
    "        th, ph: shower arrival angles.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: expected arrival time in ns.\n",
    "    \"\"\"\n",
    "    et = ((xd - x0) * torch.sin(th) * torch.cos(ph) + (yd - y0) * torch.sin(th) * torch.sin(ph)) / c0\n",
    "\n",
    "    return et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1deb9-6298-48de-94b8-7efc60c29ac6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We have some resolution in our detectors, which inherently has some uncertainty in detecting the number of particles\n",
    "#This function accounts for this uncertainty\n",
    "#We keep a fix resolution, it does not change for different flux values\n",
    "\n",
    "def SmearN(flux):\n",
    "    \"\"\"Apply detector resolution and threshold gating to expected flux values.\n",
    "\n",
    "    Parameters:\n",
    "        flux (torch.Tensor): expected number of particles.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: noisy, gated counts reflecting detector response.\n",
    "    \"\"\"\n",
    "    gate = torch.sigmoid(10 * (flux - 0.1))\n",
    "    noise = torch.randn_like(flux)  # standard normal noise\n",
    "    noisy = flux + RelResCounts * flux * noise  # reparameterized: mean + std * noise\n",
    "    \n",
    "    return gate * noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb3569-2544-4586-b245-9e3e26853d7f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TimeAverage(T, Nb, Ns):\n",
    "    \"\"\"Estimate time mean and uncertainty combining background and signal statistics.\n",
    "\n",
    "    Parameters:\n",
    "        T (torch.Tensor): expected arrival time.\n",
    "        Nb (float): background counts.\n",
    "        Ns (float): signal counts.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean, std) representing the time mean and its standard deviation used for sampling.\n",
    "    \"\"\"\n",
    "    noise = torch.rand_like(T) - .5\n",
    "    \n",
    "    if Nb <= 1:\n",
    "        STbgr = IntegrationWindow / sqrt12\n",
    "        AvTbgr = T + noise * STbgr\n",
    "\n",
    "    elif Nb <= 2:\n",
    "        STbgr = IntegrationWindow * .2041\n",
    "        AvTbgr = T + noise * STbgr\n",
    "\n",
    "    elif Nb <= 3:\n",
    "        STbgr = IntegrationWindow * .166666\n",
    "        AvTbgr = T + noise * STbgr\n",
    "\n",
    "    elif Nb <= 4:\n",
    "        STbgr = IntegrationWindow * .1445\n",
    "        AvTbgr = T + noise * STbgr\n",
    "\n",
    "    else:\n",
    "        STbgr = IntegrationWindow * .11\n",
    "        AvTbgr = torch.normal(T, STbgr)\n",
    "        \n",
    "        while (AvTbgr - T > .5 * IntegrationWindow):\n",
    "            AvTbgr = torch.normal(T, STbgr)\n",
    "\n",
    "    STsig = sigma_time\n",
    "\n",
    "    if Ns >= 2:\n",
    "        STsig = sigma_time / torch.sqrt(Ns - 1)\n",
    "\n",
    "    AvTsig = T + torch.randn_like(T) * STbgr\n",
    "\n",
    "    if Nb == 0 and Ns == 0:\n",
    "        mean = T\n",
    "        std = IntegrationWindow / sqrt12\n",
    "\n",
    "    elif Nb == 0:\n",
    "        mean = AvTsig\n",
    "        std = STsig\n",
    "\n",
    "    elif Ns == 0:\n",
    "        mean = AvTbgr\n",
    "        std = STbgr\n",
    "\n",
    "    else:\n",
    "        VTbgr = STbgr ** 2\n",
    "        VTsig = STsig ** 2\n",
    "        var = torch.sqrt(1 / VTbgr + 1 / VTsig)\n",
    "\n",
    "        mean = (AvTsig / VTsig + AvTbgr / VTsig) / var\n",
    "        std = torch.sqrt(var)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29d7aa-ed5a-41a1-8e05-2fdf27cb59fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def powerlawdist(E_min = 0.1, E_max = 10, index = 2.7):\n",
    "    \"\"\"Sample an energy from a power-law distribution using inverse transform sampling.\n",
    "\n",
    "    Parameters:\n",
    "        E_min (float): minimum energy.\n",
    "        E_max (float): maximum energy.\n",
    "        index (float): spectral index (default 2.7).\n",
    "\n",
    "    Returns:\n",
    "        float: sampled energy.\n",
    "    \"\"\"\n",
    "    # Inverse transform sampling\n",
    "    r = np.random.rand()\n",
    "    exponent = 1.0 - index\n",
    "    Emin_pow = E_min**exponent\n",
    "    Emax_pow = E_max**exponent\n",
    "    energies = (Emin_pow + r * (Emax_pow - Emin_pow)) ** (1.0 / exponent)\n",
    "    \n",
    "    return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d55a1-2202-42f9-91d5-732b49afdbdc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def symmetry_loss(x, y, n_symmetry = 3, center = (0.0, 0.0)):\n",
    "    \"\"\"Compute a loss that penalizes deviation from n-fold rotational symmetry.\n",
    "\n",
    "    The loss rotates the set of points and measures mean nearest-neighbour distance to the original set.\n",
    "\n",
    "    Parameters:\n",
    "        x, y (torch.Tensor): coordinates of detectors.\n",
    "        n_symmetry (int): order of rotational symmetry to enforce.\n",
    "        center (tuple): center point to rotate about.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: scalar symmetry loss (smaller is more symmetric).\n",
    "    \"\"\"\n",
    "    #Penalizes deviation from n-fold rotational symmetry without relying on grouping.\n",
    "    #Matches each rotated point to its nearest neighbor in the original set.\n",
    "    x_centered = x - center[0]\n",
    "    y_centered = y - center[1]\n",
    "    coords = torch.stack([x_centered, y_centered], dim=1)  # (N, 2)\n",
    "\n",
    "    sym_loss = 0.0\n",
    "    for i in range(1, n_symmetry):\n",
    "        theta = 2 * np.pi * i / n_symmetry\n",
    "        R = torch.tensor([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta),  np.cos(theta)]\n",
    "        ], dtype=coords.dtype, device=coords.device)\n",
    "\n",
    "        rotated = coords @ R.T  # (N, 2)\n",
    "\n",
    "        # Compute pairwise distances between rotated and original coords\n",
    "        dists = torch.cdist(rotated, coords, p=2)  # (N, N)\n",
    "\n",
    "        # Find nearest neighbor for each rotated point\n",
    "        min_dists, _ = dists.min(dim=1)\n",
    "\n",
    "        sym_loss += min_dists.mean()\n",
    "\n",
    "    return sym_loss / (n_symmetry - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba64c6a-8e0c-4208-afac-083d40fb3dfb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4075790-ed05-4904-a70b-97f44133fa90",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstructability(events):\n",
    "    \"\"\"Compute a differentiable reconstructability score per event.\n",
    "\n",
    "    The score uses soft thresholds to estimate whether an event has enough detector hits\n",
    "    to be reconstructable; returns a value in (0,1) per event.\n",
    "\n",
    "    Parameters:\n",
    "        events (torch.Tensor): per-event per-detector counts (shape: [Nevents, Nunits]).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: reconstructability scores per event.\n",
    "    \"\"\"\n",
    "    layout_threshold = 5. #We accept the arrays detecting >= 5 particles\n",
    "    tau_layout = 5.\n",
    "    reconstruct_threshold = 3.\n",
    "    tau_reconstruct = 5.\n",
    "\n",
    "    soft_detect = torch.sigmoid(tau_layout * (events - layout_threshold))\n",
    "    n = torch.sum(soft_detect, dim = 1)\n",
    "\n",
    "    r = torch.sigmoid(tau_reconstruct * (n - reconstruct_threshold))\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78276d01-4f0d-4ad2-ab55-6f418ce3be6d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def U_PR(r):\n",
    "    \"\"\"Utility term for reconstructability: grows with the square-root of summed reconstructability.\n",
    "\n",
    "    Parameters:\n",
    "        r (torch.Tensor): reconstructability scores per event.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: scalar utility contribution.\n",
    "    \"\"\"\n",
    "    u = torch.sqrt(torch.sum(r) + 1e-6) #1e-6 is for stability\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e844dc-6af7-414b-88d5-43e36980d33a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def U_E(E_preds, E_trues, r):\n",
    "    \"\"\"Utility term for energy performance, weighted by reconstructability.\n",
    "\n",
    "    Higher values indicate better agreement between predicted and true energies for reconstructable events.\n",
    "\n",
    "    Parameters:\n",
    "        E_preds (torch.Tensor): predicted energies (batch).\n",
    "        E_trues (torch.Tensor): true energies (batch).\n",
    "        r (torch.Tensor): reconstructability weights per event.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: scalar utility contribution.\n",
    "    \"\"\"\n",
    "    u = torch.sum(r / ((E_preds - E_trues) ** 2 + .01))\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09835b4b-e19f-4274-93eb-835af14d2969",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def U_TH(Th_preds, Th_trues, r):\n",
    "    \"\"\"Utility term for angular performance (theta), weighted by reconstructability.\n",
    "\n",
    "    Parameters:\n",
    "        Th_preds (torch.Tensor): predicted theta values.\n",
    "        Th_trues (torch.Tensor): true theta values.\n",
    "        r (torch.Tensor): reconstructability weights per event.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: scalar utility contribution.\n",
    "    \"\"\"\n",
    "    u = torch.sum(r / ((Th_preds - Th_trues) ** 2 + .00001))\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f399f13-e12b-49d1-bf2c-6447da1089b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "In this section we are creating our neural network, we will train it with some simulated EAS. the idea is to feed the network with detector positions, with detected number of particles, and time of arrival. Then we neural network output will be energy of the shower, angles, and shower core (X, Y)\n",
    "\n",
    "We have to determine the number of hidden layers and number of neurons by hand, as there is no optimization regarding these parameters.\n",
    "\n",
    "We will keep learning rate small, ie. lr = 1e-5, otherwise NN tends to overfit\n",
    "\n",
    "It might be argued that for the first part there is no reason to keep positions of the detectors as inputs in our network, but that part will be useful when we need to train our NN again for new positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fae8e7-23d5-4fb4-b164-e0af9163954e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reconstruction(nn.Module):\n",
    "    \"\"\"Fully-connected neural network to reconstruct shower properties from flattened detector inputs.\n",
    "\n",
    "    Inputs (per event) are flattened vectors of length `num_detectors * input_features` where for\n",
    "    each detector the features are `[x, y, N, T]`.\n",
    "\n",
    "    Outputs are a vector of length `output_dim` with `[X0, Y0, E_norm, Theta_norm, Phi_norm]`.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features = 4, num_detectors = 90, hidden_lay1 = 256, hidden_lay2 = 128, \n",
    "                 hidden_lay3 = 32, output_dim = 5):\n",
    "        super(Reconstruction, self).__init__()\n",
    "        self.num_detectors = num_detectors\n",
    "        self.input_features = input_features\n",
    "\n",
    "        #We have to flatten the input, since we are using fully connected neural network\n",
    "        self.L1 = nn.Linear(num_detectors * input_features, hidden_lay1)\n",
    "        self.L2 = nn.Linear(hidden_lay1, hidden_lay2)\n",
    "        self.L3 = nn.Linear(hidden_lay2, hidden_lay3)\n",
    "        self.L4 = nn.Linear(hidden_lay3, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1) #This way we drop a fraction of the neurons randomly at iteration,\n",
    "        #So that our network won't rely on some specific network path\n",
    "\n",
    "        #Output_dim = 5 means: we will have an output containing (X0, Y0, E0, Theta0, Phi0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: returns bounded outputs via `tanh` activation on top layer.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): shape (batch_size, num_detectors * input_features)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor with normalized [X0, Y0, E_norm, Theta_norm, Phi_norm].\n",
    "        \"\"\"\n",
    "        out = self.L1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.L2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.L3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.L4(out)\n",
    "        out = self.tanh(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9401c73-2941-462a-80b8-2addda57fcf4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#These functions are needed to normalize the labels and denormalize the outputs, otherwise the scales change too much that\n",
    "#NN training will fail\n",
    "def NormalizeLabels(E, theta, phi):\n",
    "    \"\"\"Normalize physical labels to the ranges expected by the network.\n",
    "\n",
    "    Parameters:\n",
    "        E, theta, phi (torch.Tensor): true energy, theta and phi.\n",
    "\n",
    "    Returns:\n",
    "        tuple: normalized (E_norm, theta_norm, phi_norm) in ranges roughly [-1,1] or equivalent.\n",
    "    \"\"\"\n",
    "    E_norm = 2 * (E - .1) / (10 - .1) - 1\n",
    "    theta_norm = 2 * theta / (theta_max) - 1\n",
    "    phi_norm = phi / torch.pi\n",
    "    \n",
    "    return E_norm, theta_norm, phi_norm\n",
    "\n",
    "def DenormalizeLabels(E_norm, theta_norm, phi_norm):\n",
    "    \"\"\"Inverse of `NormalizeLabels`: map normalized outputs back to physical units.\n",
    "\n",
    "    Parameters:\n",
    "        E_norm, theta_norm, phi_norm (torch.Tensor): normalized network outputs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: denormalized (E, theta, phi).\n",
    "    \"\"\"\n",
    "    E = 0.1 + (E_norm + 1) * (10 - 0.1) / 2\n",
    "    theta = (theta_norm + 1) * theta_max / 2\n",
    "    phi = phi_norm * torch.pi\n",
    "    \n",
    "    return E, theta, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a8606-934b-48a3-986e-ce0c2dc1abd9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We use this class in the case of early stop\n",
    "#If our network stops improving after some time there is no need to continue\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Simple early stopping helper tracking validation loss improvements.\n",
    "\n",
    "    Parameters:\n",
    "        patience (int): number of epochs with no improvement before stopping.\n",
    "        min_delta (float): minimum change to qualify as improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience = 20, min_delta = 1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e92ef7-ef43-4041-8b84-fafb36217bd3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3924e-e56c-4f02-a0fd-30128ee9e5c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Matrices we use in the functions\n",
    "# PXmg_p, PXeg_p, PXmp_p, PXep_p = ReadShowers(\"./Fit_Photon_10_pars.txt\", \"./Fit_Proton_2_pars.txt\")\n",
    "A = torch.tensor([[1, 1, 1, 1],[1, 2, 4, 8], [1, 3, 9, 27], [1, 4, 16, 64]] , dtype = torch.float32)\n",
    "\n",
    "#Constants\n",
    "c0 = .29979 #Speed of light in [m / ns] units\n",
    "theta_max = np.pi * 65 / 180\n",
    "log_01 = torch.tensor([np.log(.1)], dtype = torch.float32)\n",
    "log_10 = torch.tensor([np.log(10)], dtype = torch.float32)\n",
    "sqrt12 = torch.tensor([np.sqrt(12)], dtype = torch.float32)\n",
    "\n",
    "#Tank Values\n",
    "IntegrationWindow = 128. #128 ns integration window, SWGO default\n",
    "sigma_time = 10. #Time resolution assumed for the detectors\n",
    "R_min = 2.\n",
    "TankArea = 68.59 * np.pi #Area for 19 hexagonal macro unit\n",
    "TankRadius = np.sqrt(68.59) #Radius of macro unit\n",
    "\n",
    "#Background\n",
    "Bgr_mu_per_m2 = 0.000001826 * IntegrationWindow\n",
    "fluxB_m = torch.tensor([TankArea * Bgr_mu_per_m2])\n",
    "\n",
    "Bgr_e_per_m2 = 0.000000200 * IntegrationWindow\n",
    "fluxB_e = torch.tensor([TankArea * Bgr_e_per_m2])\n",
    "\n",
    "#Sizes\n",
    "Nunits = 500\n",
    "RelResCounts = .05\n",
    "\n",
    "#Debug Parameters\n",
    "largenumber = 1e13\n",
    "epsilon = 1 / largenumber\n",
    "\n",
    "#NN\n",
    "Nevents = 100000\n",
    "Nvalidation = 10000\n",
    "\n",
    "#Layout\n",
    "SWGOopt = False\n",
    "x, y = Layouts(n_detectors=Nunits, n_rings=9)\n",
    "\n",
    "x = torch.tensor(x, dtype = torch.float32)\n",
    "y = torch.tensor(y, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce85c2-d739-46d5-82b2-b337e12af904",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Main Routine\n",
    "We will be using here for debugging as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a2bd-504f-43bd-9567-b5d09283f42f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93111b-d517-4b62-aa9b-0727004e4118",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# energy = torch.tensor([2.], requires_grad = True)\n",
    "# theta = torch.tensor([np.pi / 6], requires_grad = True)\n",
    "# R = torch.tensor(np.linspace(0, 1000, 500), requires_grad = True)\n",
    "# fluxes = []\n",
    "\n",
    "# for r in R:\n",
    "#     fluxes.append(ShowerContent(energy, theta, r, \"mg\").item())\n",
    "\n",
    "# plt.plot(R.detach().numpy(), fluxes, linewidth = 2)\n",
    "# plt.grid()\n",
    "# plt.yscale('log')\n",
    "# plt.ylabel(\"Flux\")\n",
    "# plt.xlabel(\"R [m]\")\n",
    "# plt.xlim((0, 1000))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f5f8c-139b-4f30-b069-5b9d6411a1b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# energy = torch.tensor([2.], requires_grad = True)\n",
    "# theta = torch.tensor([np.pi / 6], requires_grad = True)\n",
    "# R = torch.tensor(np.linspace(0, 1000, 500), requires_grad = True)\n",
    "# fluxes = []\n",
    "\n",
    "# for r in R:\n",
    "#     fluxes.append(ShowerContent(energy, theta, r, \"eg\").item())\n",
    "\n",
    "# plt.plot(R.detach().numpy(), fluxes, linewidth = 2)\n",
    "# plt.grid()\n",
    "# plt.yscale('log')\n",
    "# plt.ylabel(\"Flux\")\n",
    "# plt.xlabel(\"R [m]\")\n",
    "# plt.xlim((0, 1000))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbb573-cffc-4231-be09-2ef2599881b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [6, 6])\n",
    "plt.scatter(x.detach(), y.detach(), color = \"red\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"x [m]\")\n",
    "plt.ylabel(\"y [m]\")\n",
    "plt.title(\"Initial Position of the detectors\")\n",
    "plt.xlim((-110, 110))\n",
    "plt.ylim((-110, 110))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635fdf4-6196-4622-9f86-8646c177225b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SWGOopt = False\n",
    "\n",
    "N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a = GenerateShowers(x, y, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc63bfc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"N:\", N.shape)\n",
    "print(\"T:\", T.shape)\n",
    "print(\"X0:\", X0)\n",
    "print(\"Y0:\", Y0)\n",
    "print(\"E:\", energy.shape)\n",
    "print(\"sin(theta):\", sin_z)\n",
    "print(\"cos(theta):\", cos_z)\n",
    "print(\"sin(phi):\", sin_a)\n",
    "print(\"cos(phi):\", cos_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbabd36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reconstruct theta and phi from sin and cos\n",
    "th = torch.atan2(sin_z, cos_z)\n",
    "ph = torch.atan2(sin_a, cos_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d279350-6cc4-4152-b85a-abe65f505018",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Energy = {energy.item():.3f} PeV\")\n",
    "print(f\"Angle = {th.item() * 180 / np.pi:.1f} degree\")\n",
    "\n",
    "idx = torch.where(N >= 1e-5)[0]\n",
    "norm = LogNorm(vmin = 10, vmax = torch.max(N).item())\n",
    "\n",
    "plt.figure(figsize = [7, 6])\n",
    "scatter = plt.scatter(x[idx], y[idx], c = N[idx]*10, cmap = \"viridis\", norm = norm)\n",
    "plt.colorbar(scatter, label='Number of detected particles')\n",
    "plt.scatter(X0.item(), Y0.item(), c = \"red\", marker = \"d\")\n",
    "plt.xlabel(\"x [m]\")\n",
    "plt.ylabel(\"y [m]\")\n",
    "plt.xlim((-110, 110))\n",
    "plt.ylim((-110, 110))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9e3cd-2954-4c71-84c5-635d6b28fb85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Angle = {th.item() * 180 / np.pi:.1f} degree\")\n",
    "print(f\"Phi = {ph.item() * 180 / np.pi:.1f} degree\")\n",
    "\n",
    "norm = Normalize(vmin = torch.min(T), vmax = torch.max(T))\n",
    "\n",
    "idx = torch.where(N >= 1e-5)[0]\n",
    "plt.figure(figsize = [7, 6])\n",
    "scatter = plt.scatter(x[idx], y[idx], c = T[idx], cmap = \"twilight\", norm = norm)\n",
    "plt.colorbar(scatter, label='Time of Arrival [ns]')\n",
    "plt.scatter(X0.item(), Y0.item(), c = \"green\", marker = \"d\")\n",
    "plt.xlabel(\"x [m]\")\n",
    "plt.ylabel(\"y [m]\")\n",
    "plt.xlim((-110, 110))\n",
    "plt.ylim((-110, 110))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79756ab-5eb2-4ff0-80d7-ac00c68fca45",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Training the Network\n",
    "\n",
    "We first need some simulations to train our neural network. In the global variables part, you will find a variable num_events, which is set to 10000. We will use this number to generate random events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c7262-3d34-4a3d-9083-2ef7a37a64d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Test Data Generation\n",
    "\n",
    "Number of showers in the test set = 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a6ab4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Nevents=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c16d0-79e1-4a8c-b767-5858a27c63f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inputs = torch.zeros((Nevents, Nunits, 4))\n",
    "labels = torch.zeros((Nevents, 5))\n",
    "\n",
    "for i in range(Nevents):\n",
    "    N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a = GenerateShowers(x, y)\n",
    "    # reconstruct theta and phi from sin and cos\n",
    "    th = torch.atan2(sin_z, cos_z)\n",
    "    ph = torch.atan2(sin_a, cos_a)\n",
    "\n",
    "    #Normalize the Labels:\n",
    "    E_norm, theta_norm, phi_norm = NormalizeLabels(energy, th, ph)\n",
    "    x0 = X0 / 5000 #[km]\n",
    "    y0 = Y0 / 5000 #[km]\n",
    "\n",
    "    input_vector = np.column_stack((x, y, N, T))\n",
    "\n",
    "    inputs[i] = torch.tensor(input_vector, dtype = torch.float32)\n",
    "    labels[i] = torch.tensor([x0, y0, E_norm, theta_norm, phi_norm], dtype = torch.float32)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!Shower generation is {int((i + 1) / 100)}% done!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712c8bf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"./outputs/NN_Files_3D_scaled_opt_1\", exist_ok=True)\n",
    "\n",
    "torch.save(inputs, \"./outputs/NN_Files_3D_scaled_opt_1/inputs.pt\")\n",
    "torch.save(labels, \"./outputs/NN_Files_3D_scaled_opt_1/labels.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e015-5e02-4a1e-a3e2-8a2cbda3a04b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Validation Set Data Generation\n",
    "\n",
    "Number of events in the validation set = 10k, i.e 10% of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdba768",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Nvalidation = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59aa207-03b2-4900-9711-49794854dade",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inputs_val = torch.zeros((Nvalidation, Nunits, 4))\n",
    "labels_val = torch.zeros((Nvalidation, 5))\n",
    "\n",
    "for i in range(Nvalidation):\n",
    "    N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a = GenerateShowers(x, y)\n",
    "    # reconstruct theta and phi from sin and cos\n",
    "    th = torch.atan2(sin_z, cos_z)\n",
    "    ph = torch.atan2(sin_a, cos_a)\n",
    "    \n",
    "    #Normalize the Labels:\n",
    "    E_norm, theta_norm, phi_norm = NormalizeLabels(energy, th, ph)\n",
    "    x0 = X0 / 5000 #[km]\n",
    "    y0 = Y0 / 5000 #[km]\n",
    "\n",
    "    input_vector = np.column_stack((x, y, N, T))\n",
    "\n",
    "    inputs_val[i] = torch.tensor(input_vector, dtype = torch.float32)\n",
    "    labels_val[i] = torch.tensor([x0, y0, E_norm, theta_norm, phi_norm], dtype = torch.float32)\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Shower generation is {int((i + 1) / 10)}% done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189578f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(inputs_val, \"./outputs/NN_Files_3D_scaled_opt_1/inputs_val.pt\")\n",
    "torch.save(labels_val, \"./outputs/NN_Files_3D_scaled_opt_1/labels_val.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa5dc6-0640-4add-82ba-43f054cdfc6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Load the Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f583b6e-9cef-408f-943b-5d29e85f591d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = torch.load(\"./NN_Files_3D_scaled_opt_1/inputs.pt\")\n",
    "labels = torch.load(\"./NN_Files_3D_scaled_opt_1/labels.pt\")\n",
    "\n",
    "inputs_val = torch.load(\"./NN_Files_3D_scaled_opt_1/inputs_val.pt\")\n",
    "labels_val = torch.load(\"./NN_Files_3D_scaled_opt_1/labels_val.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebba67-fb11-4d32-b888-df5545bd4318",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780a20d-8833-4664-8763-f934025e4a14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In this part we are defining our model, which is the Neural network for our reconstruction\n",
    "#We will use Mean Square Error Loss\n",
    "#Optimization model is SGD, to keep consistent with our next steps\n",
    "\n",
    "model = Reconstruction(num_detectors = Nunits)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c35dd-e0fc-49ec-9830-1fcf0d7bdf5c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, labels)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = 256, shuffle = True, drop_last = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7acc7b-a77f-4478-a1ed-8701643df1e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "val_losses = []\n",
    "losses = []\n",
    "\n",
    "early_stopper = EarlyStopping()\n",
    "\n",
    "for num_epoch in range(1000):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total_batch = 0\n",
    "    \n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        batch_size = batch_inputs.size(0)\n",
    "\n",
    "        train_x = batch_inputs.view(batch_size, -1)\n",
    "        \n",
    "        train_y = batch_labels.view(batch_size, -1)\n",
    "        \n",
    "        #Train the network\n",
    "        outputs = model(train_x)\n",
    "\n",
    "        loss = criterion(outputs, train_y)\n",
    "        epoch_loss += loss.item()\n",
    "        total_batch += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    #See if our loss in our validation set improves:\n",
    "    val_size = inputs_val.size(0)\n",
    "\n",
    "    val_x = inputs_val.view(val_size, -1)\n",
    "    val_y = labels_val.view(val_size, -1)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        val_output = model(val_x)\n",
    "        val_loss = criterion(val_output, val_y)\n",
    "    \n",
    "    val_losses.append(val_loss.item())\n",
    "    # print(val_loss.item())\n",
    "    early_stopper(val_loss)\n",
    "\n",
    "    if (num_epoch + 1) % 100 == 0:\n",
    "        print(f\"Training is {int((num_epoch + 1) / 10)}% done, with Loss = {val_loss:.2f}\")\n",
    "\n",
    "    losses.append(epoch_loss / total_batch)\n",
    "\n",
    "    # if early_stopper.early_stop:\n",
    "    #     print(f\"Early stop at epoch {num_epoch + 1}\")\n",
    "    #     print(f\"Validation loss = {val_loss:.2f}\")\n",
    "    #     print(f\"Training loss = {epoch_loss / total_batch:.2f}\")\n",
    "        # break\n",
    "\n",
    "plt.plot(np.arange(1, len(losses) + 1), losses, color = \"blue\", label = \"Training Loss\")\n",
    "plt.plot(np.arange(1, len(val_losses) + 1), val_losses, color = \"red\", label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b09984",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./outputs/NN_Files_3D_scaled_opt_1/model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad0cad-499a-4a7e-9b5c-2594651ad21d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./outputs/NN_Files_3D_scaled_opt_1/model_weights.pth\"))\n",
    "\n",
    "E_r = []\n",
    "E_p = []\n",
    "\n",
    "X_r = []\n",
    "X_p = []\n",
    "\n",
    "Y_r = []\n",
    "Y_p = []\n",
    "\n",
    "Th_r = []\n",
    "Th_p = []\n",
    "\n",
    "Ph_r = []\n",
    "Ph_p = []\n",
    "\n",
    "for i in range(1000):\n",
    "    N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a = GenerateShowers(x, y)\n",
    "    # reconstruct theta and phi from sin and cos\n",
    "    th = torch.atan2(sin_z, cos_z)\n",
    "    ph = torch.atan2(sin_a, cos_a)\n",
    "    \n",
    "    #Normalize the Labels:\n",
    "    E_norm, theta_norm, phi_norm = NormalizeLabels(energy, th, ph)\n",
    "    x0 = X0 / 5000 #[km]\n",
    "    y0 = Y0 / 5000 #[km]\n",
    "    \n",
    "    X = torch.tensor(np.column_stack((x, y, N, T)), dtype = torch.float32)\n",
    "    X = X.view(1, -1)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "\n",
    "    x_pred = output[0, 0]\n",
    "    y_pred = output[0, 1]\n",
    "    E_pred = output[0, 2]\n",
    "\n",
    "    theta_pred = output[0, 3]\n",
    "    phi_pred = output[0, 4]\n",
    "    E_pred, theta_pred, phi_pred = DenormalizeLabels(E_pred, theta_pred, phi_pred)\n",
    "    \n",
    "    E_p.append(E_pred)\n",
    "    E_r.append(energy.item())\n",
    "\n",
    "    X_r.append(x0.item())\n",
    "    X_p.append(x_pred * 5000)\n",
    "\n",
    "    Y_r.append(y0.item())\n",
    "    Y_p.append(y_pred * 5000)\n",
    "\n",
    "    Th_r.append(th.item())\n",
    "    Th_p.append(theta_pred)\n",
    "\n",
    "    Ph_r.append(ph.item())\n",
    "    Ph_p.append(phi_pred)\n",
    "\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Predicted: {E_pred:.1f}, Real: {energy.item():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d42eec-b0cc-4d90-afd7-197db29f3361",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "E_max_plt = max(torch.abs(torch.tensor(E_r))) * 1.1\n",
    "plt.scatter(E_r, E_p, alpha = .5, color = \"r\")\n",
    "plt.plot(np.arange(.0, E_max_plt, .01), np.arange(.0, E_max_plt, .01), color = \"black\", linewidth = 4)\n",
    "plt.xlabel(\"True E\")\n",
    "plt.ylabel(\"Predicted E\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8c0c3-a787-4f2e-a18a-8b0ff1cebc49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_max_plt = (torch.abs(torch.tensor(X_r))*5000).max().item()\n",
    "plt.scatter(torch.tensor(X_r)*5000, X_p, alpha = .5, color = \"r\")\n",
    "plt.plot(np.arange(0, X_max_plt, 2), np.arange(0, X_max_plt, 2), color = \"black\", linewidth = 4)\n",
    "plt.xlabel(\"True X\")\n",
    "plt.ylabel(\"Predicted X\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f5a2b-bdb5-4146-8462-69ec7ecb4ff0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_max_plt = (torch.abs(torch.tensor(Y_r))*5000).max().item()\n",
    "plt.scatter(torch.tensor(Y_r), Y_p, alpha = .5, color = \"r\")\n",
    "plt.plot(np.arange(0, Y_max_plt, 2), np.arange(0, Y_max_plt, 2), color = \"black\", linewidth = 4)\n",
    "plt.xlabel(\"True Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2279c79-3936-4b7d-a787-ecee0fb0dc34",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(Th_r, Th_p, alpha = .5, color = \"r\")\n",
    "plt.plot(np.arange(0, np.pi * 65 / 180, .1), np.arange(0, np.pi * 65 / 180, .1), color = \"black\", linewidth = 4)\n",
    "plt.xlabel(\"True Theta\")\n",
    "plt.ylabel(\"Predicted Theta\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446a446-f9c3-440b-b562-332a72b150d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(Ph_r, Ph_p, alpha = .5, color = \"r\")\n",
    "plt.plot(np.arange(- np.pi, np.pi, .1), np.arange(- np.pi, np.pi, .1), color = \"black\", linewidth = 4)\n",
    "plt.xlabel(\"True Phi\")\n",
    "plt.ylabel(\"Predicted Phi\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5693d7-a936-4ed9-98a4-de17566398ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.abs(np.array(Y_p) - np.array(Y_r))\n",
    "idx_low = np.where(np.array(E_r) <= 0.3)\n",
    "idx_mid = np.where((np.array(E_r) <= 0.5) & (np.array(E_r) > 0.3))\n",
    "idx_high = np.where(np.array(E_r) > 0.5)\n",
    "\n",
    "bin_high = np.logspace(np.log10(min(data[idx_high])), np.log10(max(data[idx_high])), 50)\n",
    "bin_mid = np.logspace(np.log10(min(data[idx_mid])), np.log10(max(data[idx_mid])), 50)\n",
    "bin_low = np.logspace(np.log10(min(data[idx_low])), np.log10(max(data[idx_low])), 50)\n",
    "\n",
    "plt.hist(data[idx_high], bins = bin_high, alpha = .5, color = \"blue\", label = \"High Energy Events\")\n",
    "plt.hist(data[idx_mid], bins = bin_mid, alpha = .5, color = \"yellow\", label = \"Medium Energy Events\")\n",
    "plt.hist(data[idx_low], bins = bin_low, alpha = .5, color = \"red\", label = \"Low Energy Events\")\n",
    "plt.xlabel(\"Core Position Y Loss\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc66b0-e138-4725-b03e-dbc430a491e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Optimization\n",
    "\n",
    "This is part where we start moving our arrays according to Utility Function.\n",
    "\n",
    "You can check the utilities that we use above and see that it is > 0\n",
    "\n",
    "So we will use -U as a \"loss\" function and minimize it, so that we can maximize our value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41806008-8a1d-4d26-98f5-9e95451650cf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#First save the weights or load them so that we don't have to train the network every time we start the kernel\n",
    "#Remove the commend operator \"#\" if you want to save\n",
    "\n",
    "#torch.save(model.state_dict(), './NN_files/model_weights.pth')\n",
    "\n",
    "model = Reconstruction()\n",
    "\n",
    "path = Path(\"./NN_Files/checkpoint.pth\")\n",
    "\n",
    "if path.exists():\n",
    "    checkpoint = torch.load('./NN_Files/checkpoint.pth')\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Updated Weights are loaded\")\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load('./NN_Files/model_weights.pth'))\n",
    "    print(\"Initial Weights are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dcdaa-eda8-4fa0-8182-e1cc0dcd997b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def barycentric_coords(P, A, B, C):\n",
    "    \"\"\"\n",
    "    Compute barycentric coordinates for each point P with respect to triangle ABC.\n",
    "    P: Tensor of shape (N, 2)\n",
    "    A, B, C: Tensors of shape (2,)\n",
    "    \"\"\"\n",
    "    v0 = C - A\n",
    "    v1 = B - A\n",
    "    v2 = P - A\n",
    "\n",
    "    d00 = v0 @ v0\n",
    "    d01 = v0 @ v1\n",
    "    d11 = v1 @ v1\n",
    "    d20 = torch.sum(v2 * v0, dim=1)\n",
    "    d21 = torch.sum(v2 * v1, dim=1)\n",
    "\n",
    "    denom = d00 * d11 - d01 * d01 + 1e-8\n",
    "    u = (d11 * d20 - d01 * d21) / denom\n",
    "    v = (d00 * d21 - d01 * d20) / denom\n",
    "    return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697ba07-48d8-4507-bbf4-061fc8d5b343",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def project_to_triangle(x, y):\n",
    "    \"\"\"\n",
    "    Projects each (x[i], y[i]) point inside the triangle defined by:\n",
    "    A = (-3200, 2000), B = (1800, 2000), C = (1800, -3600)\n",
    "    x, y: tensors of shape (N,)\n",
    "    Returns: projected x and y, tensors of shape (N,)\n",
    "    \"\"\"\n",
    "    A = torch.tensor([-3800.0, 1500.0], device=x.device)\n",
    "    B = torch.tensor([1200.0, 1500.0], device=x.device)\n",
    "    C = torch.tensor([1200.0, -4100.0], device=x.device)\n",
    "\n",
    "    P = torch.stack([x, y], dim=1)  # (N, 2)\n",
    "\n",
    "    # Compute barycentric coordinates\n",
    "    u, v = barycentric_coords(P, A, B, C)\n",
    "\n",
    "    # Determine which points are inside the triangle\n",
    "    inside = (u >= 0) & (v >= 0) & (u + v <= 1)\n",
    "\n",
    "    # Clip to triangle: u, v  [0, 1], u + v  1\n",
    "    u_clipped = torch.clamp(u, 0.0, 1.0)\n",
    "    v_clipped = torch.clamp(v, 0.0, 1.0)\n",
    "    uv_sum = u_clipped + v_clipped\n",
    "    over = uv_sum > 1.0\n",
    "    u_clipped[over] = u_clipped[over] / uv_sum[over]\n",
    "    v_clipped[over] = v_clipped[over] / uv_sum[over]\n",
    "\n",
    "    v0 = C - A\n",
    "    v1 = B - A\n",
    "    P_proj = A + u_clipped.unsqueeze(1) * v0 + v_clipped.unsqueeze(1) * v1\n",
    "\n",
    "    # If already inside, keep P. Otherwise, use projection.\n",
    "    final_P = torch.where(inside.unsqueeze(1), P, P_proj)\n",
    "\n",
    "    return final_P[:, 0], final_P[:, 1]  # x_proj, y_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12a1f4-95f5-4c9f-9df8-80d3575ed097",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "e = 0\n",
    "os.makedirs(\"./Python_Layout\", exist_ok=True)\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    p_layout = Path(f\"./Python_Layout/Layout_{i + 1}.txt\")\n",
    "    \n",
    "    if p_layout.exists():\n",
    "        data = np.loadtxt(p_layout)\n",
    "        \n",
    "        x = torch.tensor(data[:, 0], dtype = torch.float32)\n",
    "        y = torch.tensor(data[:, 1], dtype = torch.float32)\n",
    "        e = i + 1\n",
    "\n",
    "if e > 0:\n",
    "    print(f\"Updated Layout {e} is initialized\")\n",
    "\n",
    "else:\n",
    "    print(\"First Layout is initialized\")\n",
    "\n",
    "class LearnableXY(torch.nn.Module):\n",
    "    \"\"\"Small module holding detector x,y positions as learnable parameters.\n",
    "\n",
    "    The parameters can be optimized with standard PyTorch optimizers to change the layout.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_init, y_init):\n",
    "        super().__init__()\n",
    "        self.x = torch.nn.Parameter(x_init)\n",
    "        self.y = torch.nn.Parameter(y_init)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Return current learnable coordinates as (x, y).\"\"\"\n",
    "        return self.x, self.y\n",
    "\n",
    "xy_module = LearnableXY(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e23be-95d7-4b89-9ad9-4c4ec11523f8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def push_apart(module, min_dist = 2 * TankRadius):\n",
    "    \"\"\"Enforce a minimum separation between detectors by small pairwise displacements.\n",
    "\n",
    "    Parameters:\n",
    "        module (LearnableXY): module exposing `x` and `y` parameters via forward().\n",
    "        min_dist (float): desired minimum separation between detectors.\n",
    "    \"\"\"\n",
    "    x, y = module()  # Correctly calls forward()\n",
    "    coords = torch.stack([x, y], dim=1)  # shape (N, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(coords.shape[0]):\n",
    "            diffs = coords[i] - coords\n",
    "            dists = torch.norm(diffs, dim=1)\n",
    "            mask = (dists < min_dist) & (dists > 0)\n",
    "\n",
    "            for j in torch.where(mask)[0]:\n",
    "                direction = diffs[j] / dists[j]\n",
    "                displacement = 0.5 * (min_dist - dists[j]) * direction\n",
    "                coords[i] += displacement\n",
    "                coords[j] -= displacement\n",
    "\n",
    "        # Update learnable parameters in-place\n",
    "        module.x.data.copy_(coords[:, 0])\n",
    "        module.y.data.copy_(coords[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edb66f-5695-4882-8c49-f1a3a6e78524",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Now we do the optimization step\n",
    "SWGOopt = True\n",
    "optimizer = torch.optim.SGD(xy_module.parameters(), lr = 5, momentum = .9)\n",
    "\n",
    "if path.exists():\n",
    "    checkpoint = torch.load(path)  # or map_location=... if needed\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    num_epoch = checkpoint.get(\"epoch\") + 1\n",
    "    loss = checkpoint.get(\"loss\")\n",
    "    \n",
    "    print(f\"Optimizer is initialized from the last epoch {num_epoch}\")\n",
    "\n",
    "else:\n",
    "    num_epoch = 0\n",
    "    print(\"Optimizer is initialized\")\n",
    "\n",
    "max_grad = 10.\n",
    "\n",
    "Nbatch = 100#50\n",
    "\n",
    "if num_epoch < 20:\n",
    "    Nfinetune = 25\n",
    "\n",
    "elif num_epoch >= 20:\n",
    "    Nfinetune = 50\n",
    "\n",
    "U_vals = []\n",
    "U_pr_vals = []\n",
    "U_e_vals = []\n",
    "U_th_vals = []\n",
    "\n",
    "for epoch in range(num_epoch, num_epoch + 200):\n",
    "    x1, y1 = xy_module()\n",
    "    \n",
    "    #Batch Generation:\n",
    "    N_list = []\n",
    "    T_list = []\n",
    "    labels_batch = torch.zeros((Nbatch, 5))\n",
    "    \n",
    "    for i in range(Nbatch):\n",
    "        N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a = GenerateShowers(x1, y1)\n",
    "        # reconstruct theta and phi from sin and cos\n",
    "        th = torch.atan2(sin_z, cos_z)\n",
    "        ph = torch.atan2(sin_a, cos_a)\n",
    "\n",
    "        N_list.append(N)\n",
    "        T_list.append(T)\n",
    "        labels_batch[i] = torch.tensor([X0, Y0, energy, th, ph], dtype = torch.float32)\n",
    "    \n",
    "    x = x1.unsqueeze(0).repeat(Nbatch, 1)\n",
    "    y = y1.unsqueeze(0).repeat(Nbatch, 1)\n",
    "\n",
    "    events_n_batch = torch.stack(N_list).squeeze(-1)  # shape: (Nbatch, Nunits)\n",
    "    events_t_batch = torch.stack(T_list).squeeze(-1)\n",
    "    \n",
    "    events_batch = torch.stack((x, y, events_n_batch, events_t_batch), dim = 2)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    preds_batch = model(events_batch.view(Nbatch, -1))\n",
    "\n",
    "    #Denormalize the predictions\n",
    "    preds_x = preds_batch[:, 0] * 5000\n",
    "    preds_y = preds_batch[:, 1] * 5000\n",
    "    preds_e, preds_th, preds_phi = DenormalizeLabels(preds_batch[:, 2], preds_batch[:, 3], \n",
    "                                                                                preds_batch[:, 4])\n",
    "\n",
    "    #Compute the reconstructability score for each event:\n",
    "    r_score = reconstructability(events_batch[:, :, 2])\n",
    "    density = Nbatch / ((labels_batch[:, 0].max() - labels_batch[:, 0].min()) * \n",
    "                        (labels_batch[:, 1].max() - labels_batch[:, 1].min()))\n",
    "\n",
    "    #Compute Utility:\n",
    "    U = 1e-2 * U_TH(preds_th, labels_batch[:, 3], r_score) + U_E(preds_e, labels_batch[:, 2], r_score) + U_PR(r_score) / torch.sqrt(density)\n",
    "\n",
    "    #Save the utility values to plot\n",
    "    U_vals.append(U.item())\n",
    "    U_pr_vals.append((U_PR(r_score) / torch.sqrt(density)).item())\n",
    "    U_e_vals.append(U_E(preds_e, labels_batch[:, 2], r_score).item())\n",
    "    U_th_vals.append(1e-2 * U_TH(preds_th, labels_batch[:, 3], r_score).item())\n",
    "\n",
    "    #sym_loss = symmetry_loss(x1, y1, n_symmetry = 3)\n",
    "    print(f\"Utility: {U:.2f}\")\n",
    "\n",
    "    #Use utility as - Loss, we have a penalty term to make configuration symmetric\n",
    "    Loss = - U #+ lambda_sym * sym_loss\n",
    "\n",
    "    Loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(xy_module.parameters(), max_norm = max_grad)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        push_apart(xy_module)\n",
    "\n",
    "    #This part is done to keep the detectors inside the site\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_proj, y_proj = project_to_triangle(x1.view(-1), y1.view(-1))\n",
    "        x1.copy_(x_proj)\n",
    "        y1.copy_(y_proj)\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Now we need to generate new events that with the new layout so that we can fine tune our NN when it's necessary\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Fine Tune at epoch {epoch + 1}\")\n",
    "        \n",
    "        finetune_events = torch.zeros((Nfinetune, Nunits, 4))\n",
    "        finetune_trues = torch.zeros((Nfinetune, 5))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x2, y2 = xy_module()\n",
    "            x = x2.detach()\n",
    "            y = y2.detach()\n",
    "\n",
    "            #Generate events to fine tune the network\n",
    "            for i in range(Nfinetune):\n",
    "                N, T, X0, Y0, energy, sin_z, cos_z, sin_a, cos_a = GenerateShowers(x, y)\n",
    "                # reconstruct theta and phi from sin and cos\n",
    "                theta = torch.atan2(sin_z, cos_z)\n",
    "                phi = torch.atan2(sin_a, cos_a)\n",
    "\n",
    "                X0 /= 5000\n",
    "                Y0 /= 5000\n",
    "                energy, theta, phi = NormalizeLabels(energy, theta, phi)\n",
    "\n",
    "                finetune_events[i] = torch.tensor(np.column_stack((x, y, N, T)))\n",
    "                finetune_trues[i] = torch.tensor([X0, Y0, energy, theta, phi], dtype = torch.float32)\n",
    "\n",
    "        ReconstructionNN = model\n",
    "        ReconstructionNN.train()\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizerNN = torch.optim.Adam(ReconstructionNN.parameters(), lr = 5e-5)\n",
    "\n",
    "        dataset = TensorDataset(finetune_events, finetune_trues)\n",
    "        dataloader = DataLoader(dataset, batch_size = 32, shuffle = True, drop_last = True, num_workers = 4)\n",
    "\n",
    "        for j in range(5):\n",
    "            for ft_batch, ft_trues in dataloader:\n",
    "                batch_size = ft_batch.size(0)\n",
    "\n",
    "                train_x = ft_batch.view(batch_size, -1)\n",
    "        \n",
    "                train_y = ft_trues.view(batch_size, 5)\n",
    "        \n",
    "                #Train the network\n",
    "                outputs = ReconstructionNN(train_x)\n",
    "\n",
    "                lossT = criterion(outputs, train_y)\n",
    "\n",
    "                lossT.backward()\n",
    "                optimizerNN.step()\n",
    "\n",
    "                optimizerNN.zero_grad()\n",
    "\n",
    "    #I will save the layouts and weights here so that I can stop whenever and continue later\n",
    "    torch.save({\"epoch\": epoch, \"loss\": Loss, \"model_state_dict\": model.state_dict(), \n",
    "                \"optimizer_state_dict\": optimizer.state_dict()}, \"./NN_Files/checkpoint.pth\")\n",
    "    \n",
    "    np.savetxt(f\"./Python_Layout/Layout_{epoch + 1}.txt\", np.column_stack((x1.detach().numpy(), y1.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfe8ac-f5aa-4826-ad16-516bba433df4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a, b = Layouts()\n",
    "\n",
    "plt.figure(figsize = [6, 6])\n",
    "plt.scatter(x.detach(), y.detach(), color = \"red\", alpha = .8, label = \"Final Layout\")\n",
    "plt.scatter(a, b, color = \"blue\", alpha = .3, label = \"Initial Layout\")\n",
    "#plt.plot([-3800, 1200, 1200, -3800], [1500, 1500, -4100, 1500], \"k--\", label = \"SWGO Site\")\n",
    "\n",
    "#Add Shower Simulation Area\n",
    "#ax = plt.gca()\n",
    "#rect = patches.Rectangle((-3000, -3000), 6000, 6000, linewidth=1.5, edgecolor='green', facecolor='green',\n",
    "                         #alpha=0.1, label='Shower Simulation Area')\n",
    "#ax.add_patch(rect)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"x [m]\")\n",
    "plt.ylabel(\"y [m]\")\n",
    "plt.title(\"Position of the detectors\")\n",
    "plt.legend()\n",
    "plt.xlim((-3000, 3000))\n",
    "plt.ylim((-3000, 3000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e98c5-a7fc-4f43-80ed-2c202ccdddf8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utility_path = Path(\"./Python_Layout/Utilities.txt\")\n",
    "\n",
    "if utility_path.exists():\n",
    "    data = np.loadtxt(utility_path)\n",
    "    u_t = data\n",
    "    #u_p = data[:, 1]\n",
    "    #u_e = data[:, 2]\n",
    "    #u_a = data[:, 3]\n",
    "\n",
    "    u_t = np.append(u_t, np.array(U_vals).ravel())\n",
    "    #u_p = np.append(u_p, np.array(U_pr_vals).ravel())\n",
    "    #u_e = np.append(u_e, np.array(U_e_vals).ravel())\n",
    "    #u_a = np.append(u_a, np.array(U_th_vals).ravel())\n",
    "\n",
    "    #data = np.column_stack((u_t, u_p, u_e, u_a))\n",
    "\n",
    "    np.savetxt(utility_path, u_t)\n",
    "    \n",
    "else:\n",
    "    u_t = U_vals\n",
    "    #u_p = U_pr_vals\n",
    "    #u_e = U_e_vals\n",
    "    #u_a = U_th_vals\n",
    "    \n",
    "    #data = np.column_stack((u_t, u_p, u_e, u_a))\n",
    "    np.savetxt(utility_path, u_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204d404-53ff-4b4e-8ba2-ce783af0818b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_ut = [np.mean(u_t[i - 4: i]) for i in range(4, len(u_t))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3a02d-9254-4599-a24c-e6ccace68dc2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(u_t, color = \"black\", linestyle = \"none\", marker = \"o\", linewidth = .5, label = \"Total Utility\")\n",
    "plt.plot(np.arange(4, len(u_t)), mean_ut, linestyle = \"-\", linewidth = 2, label = \"Mean Utility\")\n",
    "#plt.plot(u_p, color = \"orange\", linestyle = \"-.\", label = \"Reconstructability Utility\")\n",
    "#plt.plot(u_e, color = \"green\", linestyle = \"-.\", label = \"Energy Utility\")\n",
    "#plt.plot(u_a, color = \"purple\", linestyle = \"-.\", label = \"Angle Utility\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Utility Score\")\n",
    "plt.ylim((0, np.max(u_t) + 1000))\n",
    "plt.title(\"Utility Values per Optimization Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424e393",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0f6fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiproc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "SWGOLO7_optimization.ipynb",
   "output_path": "outputs_notebooks/SWGOLO7_optimization_output_20260203_085901.ipynb",
   "parameters": {},
   "start_time": "2026-02-03T13:59:01.330964",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}